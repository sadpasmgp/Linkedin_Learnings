{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bd28635",
   "metadata": {},
   "source": [
    "### Normalization vs Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4522d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17c18bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db847f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = spark.createDataFrame([\n",
    "    (1, Vectors.dense([10.0, 10000.0, 1.0]),),\n",
    "    (2, Vectors.dense([20.0, 30000.0, 2.0]),),\n",
    "    (3, Vectors.dense([30.0, 60000.0, 3.0]),)],\n",
    "     [\"id\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7d4d2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| id|          features|\n",
      "+---+------------------+\n",
      "|  1|[10.0,10000.0,1.0]|\n",
      "|  2|[20.0,30000.0,2.0]|\n",
      "|  3|[30.0,60000.0,3.0]|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dc06eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scaler = MinMaxScaler(inputCol=\"features\",\n",
    "                             outputCol=\"sfeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f02b7fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "smodel = feature_scaler.fit(features_df)\n",
    "sfeatures_df = smodel.transform(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dbe7953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-------------+\n",
      "| id|          features|    sfeatures|\n",
      "+---+------------------+-------------+\n",
      "|  1|[10.0,10000.0,1.0]|    (3,[],[])|\n",
      "|  2|[20.0,30000.0,2.0]|[0.5,0.4,0.5]|\n",
      "|  3|[30.0,60000.0,3.0]|[1.0,1.0,1.0]|\n",
      "+---+------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sfeatures_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57332d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8606d265",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_stand_scaler = StandardScaler(inputCol=\"features\", \n",
    "                                     outputCol=\"stdfeatures\",\n",
    "                                     withStd=True, withMean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3856faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdmodel = feature_stand_scaler.fit(features_df)\n",
    "stdfeatures_df = stdmodel.transform(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93a4e85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+--------------------+\n",
      "| id|          features|         stdfeatures|\n",
      "+---+------------------+--------------------+\n",
      "|  1|[10.0,10000.0,1.0]|[-1.0,-0.92717264...|\n",
      "|  2|[20.0,30000.0,2.0]|[0.0,-0.132453235...|\n",
      "|  3|[30.0,60000.0,3.0]|[1.0,1.0596258856...|\n",
      "+---+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stdfeatures_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23989fa",
   "metadata": {},
   "source": [
    "### Bucketizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f105cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08458d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [-float(\"inf\"), -10.0, 0.0, 10.0, float(\"inf\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "517ed5e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-800.0,), (-10.5,), (-1.7,), (0.0,), (8.2,), (90.1,)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_data = [(-800.0,),(-10.5,),(-1.7,), (0.0,), (8.2,), (90.1,)]\n",
    "b_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7d8fc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|features|\n",
      "+--------+\n",
      "|  -800.0|\n",
      "|   -10.5|\n",
      "|    -1.7|\n",
      "|     0.0|\n",
      "|     8.2|\n",
      "|    90.1|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "b_df = spark.createDataFrame(b_data, [\"features\"])\n",
    "b_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "611645ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|features|bfeatures|\n",
      "+--------+---------+\n",
      "|  -800.0|      0.0|\n",
      "|   -10.5|      0.0|\n",
      "|    -1.7|      1.0|\n",
      "|     0.0|      2.0|\n",
      "|     8.2|      2.0|\n",
      "|    90.1|      3.0|\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bucketizer = Bucketizer(splits=splits, inputCol=\"features\", outputCol=\"bfeatures\")\n",
    "bucketed_df = bucketizer.transform(b_df)\n",
    "bucketed_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea4bb07",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2cec805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c027ef04",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df = spark.createDataFrame([\n",
    "    (1, \"This is an introduction to Spark MLlib\"),\n",
    "    (2, \"MLlib includes libraries for classification and Regression\"),\n",
    "    (3, \"It also contains supporting tools for pipelines\")\n",
    "], [\"id\", \"sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62c42e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|           sentences|\n",
      "+---+--------------------+\n",
      "|  1|This is an introd...|\n",
      "|  2|MLlib includes li...|\n",
      "|  3|It also contains ...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9388aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_token = Tokenizer(inputCol=\"sentences\", outputCol=\"words\")\n",
    "sent_tokenized_df = sent_token.transform(sentences_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97a5fc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|           sentences|               words|\n",
      "+---+--------------------+--------------------+\n",
      "|  1|This is an introd...|[this, is, an, in...|\n",
      "|  2|MLlib includes li...|[mllib, includes,...|\n",
      "|  3|It also contains ...|[it, also, contai...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sent_tokenized_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a52860",
   "metadata": {},
   "source": [
    "### TF-IDF (Term frequency - inverse document frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7d919a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cfde4d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|           sentences|\n",
      "+---+--------------------+\n",
      "|  1|This is an introd...|\n",
      "|  2|MLlib includes li...|\n",
      "|  3|It also contains ...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1a138029",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"words\", \n",
    "                      outputCol=\"rawFeatures\",\n",
    "                     numFeatures=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "01fd97ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_hfTF_df = hashingTF.transform(sent_tokenized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "42967257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+\n",
      "| id|           sentences|               words|         rawFeatures|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "|  1|This is an introd...|[this, is, an, in...|(20,[6,8,9,10,13,...|\n",
      "|  2|MLlib includes li...|[mllib, includes,...|(20,[2,4,11,12,15...|\n",
      "|  3|It also contains ...|[it, also, contai...|(20,[1,4,6,8,11,1...|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sent_hfTF_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3e35d62d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, sentences='This is an introduction to Spark MLlib', words=['this', 'is', 'an', 'introduction', 'to', 'spark', 'mllib'], rawFeatures=SparseVector(20, {6: 2.0, 8: 1.0, 9: 1.0, 10: 1.0, 13: 1.0, 15: 1.0}))]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_hfTF_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "819862df",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"rawFeatures\", \n",
    "         outputCol=\"idf_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d282a3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "| id|           sentences|               words|         rawFeatures|        idf_features|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "|  1|This is an introd...|[this, is, an, in...|(20,[6,8,9,10,13,...|(20,[6,8,9,10,13,...|\n",
      "|  2|MLlib includes li...|[mllib, includes,...|(20,[2,4,11,12,15...|(20,[2,4,11,12,15...|\n",
      "|  3|It also contains ...|[it, also, contai...|(20,[1,4,6,8,11,1...|(20,[1,4,6,8,11,1...|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idfModel = idf.fit(sent_hfTF_df)\n",
    "\n",
    "tfidf_df = idfModel.transform(sent_hfTF_df)\n",
    "tfidf_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e499f984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, sentences='This is an introduction to Spark MLlib', words=['this', 'is', 'an', 'introduction', 'to', 'spark', 'mllib'], rawFeatures=SparseVector(20, {6: 2.0, 8: 1.0, 9: 1.0, 10: 1.0, 13: 1.0, 15: 1.0}), idf_features=SparseVector(20, {6: 0.5754, 8: 0.2877, 9: 0.6931, 10: 0.6931, 13: 0.6931, 15: 0.2877}))]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f571cea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427c3530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9293b0bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
